{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d72bc68-6071-4c37-979e-173861ca1da7",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### **Building Forecast-Ready Dataset with Weather & Metadata**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de37ef0-e03e-4a7e-b1e2-ed9348f4bb25",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-06T23:09:27.4636223Z",
       "execution_start_time": "2025-08-06T23:09:23.1839102Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "f8f51527-103c-44bc-8622-a623b99d40a2",
       "queued_time": "2025-08-06T23:09:11.6022527Z",
       "session_id": "da73cb9d-a63b-4402-8289-ae75c02491ac",
       "session_start_time": "2025-08-06T23:09:11.6032517Z",
       "spark_pool": null,
       "state": "finished",
       "statement_id": 3,
       "statement_ids": [
        3
       ]
      },
      "text/plain": [
       "StatementMeta(, da73cb9d-a63b-4402-8289-ae75c02491ac, 3, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random, numpy as np, os\n",
    "import json\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d845cf1e-22d3-486e-8ea7-9b5c655ab2da",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-06T23:09:27.8909694Z",
       "execution_start_time": "2025-08-06T23:09:27.4659466Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "36f71263-36e1-4216-b3af-249171906396",
       "queued_time": "2025-08-06T23:09:11.6060602Z",
       "session_id": "da73cb9d-a63b-4402-8289-ae75c02491ac",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 4,
       "statement_ids": [
        4
       ]
      },
      "text/plain": [
       "StatementMeta(, da73cb9d-a63b-4402-8289-ae75c02491ac, 4, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set seed\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "os.environ['PYTHONHASHSEED'] = '42'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f401d1b4-e4c2-4752-8869-f0b06198834a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-06T23:09:53.0758399Z",
       "execution_start_time": "2025-08-06T23:09:27.8931785Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "8c29acd1-99ce-41e2-be0a-42b2cef59dd6",
       "queued_time": "2025-08-06T23:09:11.6098814Z",
       "session_id": "da73cb9d-a63b-4402-8289-ae75c02491ac",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 5,
       "statement_ids": [
        5
       ]
      },
      "text/plain": [
       "StatementMeta(, da73cb9d-a63b-4402-8289-ae75c02491ac, 5, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datetime column set as index.\n"
     ]
    }
   ],
   "source": [
    "# Load data into pandas DataFrame \n",
    "imputed_fact_live_times = spark.read.table(\"imputed_fact_live_times\").toPandas()\n",
    "\n",
    "# Ensure Datetime is set as index\n",
    "if not isinstance(imputed_fact_live_times.index, pd.DatetimeIndex):\n",
    "    if 'Datetime' in imputed_fact_live_times.columns:\n",
    "        imputed_fact_live_times = imputed_fact_live_times.set_index('Datetime').sort_index()\n",
    "        print(\"Datetime column set as index.\")\n",
    "    else:\n",
    "        raise ValueError(\"The dataframe does not have a 'Datetime' column to set as index.\")\n",
    "else:\n",
    "    print(\"Datetime index already set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c047e3b-9b43-465e-af3a-e4a986b8873a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-06T23:09:57.8807377Z",
       "execution_start_time": "2025-08-06T23:09:53.0782895Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "c732e4c7-b112-411d-94de-5627f7e12749",
       "queued_time": "2025-08-06T23:09:11.6137242Z",
       "session_id": "da73cb9d-a63b-4402-8289-ae75c02491ac",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 6,
       "statement_ids": [
        6
       ]
      },
      "text/plain": [
       "StatementMeta(, da73cb9d-a63b-4402-8289-ae75c02491ac, 6, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weather_forecast_df = (\n",
    "    spark.read.table(\"stg_weather_forecast\")\n",
    "    .select(\"Datetime\", \"Temp\", \"IsRaining\", \"IsSnowing\")\n",
    "    .toPandas()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e68e0f9-34d8-4649-8ba0-380e01b64780",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-06T23:09:58.2858857Z",
       "execution_start_time": "2025-08-06T23:09:57.8833537Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "3ca63967-5e44-44ba-b2f6-283966dc3cfa",
       "queued_time": "2025-08-06T23:09:11.6172773Z",
       "session_id": "da73cb9d-a63b-4402-8289-ae75c02491ac",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 7,
       "statement_ids": [
        7
       ]
      },
      "text/plain": [
       "StatementMeta(, da73cb9d-a63b-4402-8289-ae75c02491ac, 7, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the latest datetime from training data index\n",
    "max_train_time = imputed_fact_live_times.index.max()\n",
    "\n",
    "# Ensure Datetime is datetime type\n",
    "weather_forecast_df['Datetime'] = pd.to_datetime(weather_forecast_df['Datetime'])\n",
    "\n",
    "# Filter to park operational hours\n",
    "weather_forecast_df['Hour'] = weather_forecast_df['Datetime'].dt.hour\n",
    "weather_forecast_df = weather_forecast_df[\n",
    "    (weather_forecast_df['Hour'] >= 6) & (weather_forecast_df['Hour'] <= 22)\n",
    "]\n",
    "\n",
    "# Filter to only include hours after training data\n",
    "weather_forecast_df = weather_forecast_df[weather_forecast_df['Datetime'] > max_train_time]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34d8741-4609-4568-a576-3b84f6efe42f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-06T23:09:58.6623179Z",
       "execution_start_time": "2025-08-06T23:09:58.2882134Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "6812a0fa-c5fd-4436-9bea-1c3a6b0d6336",
       "queued_time": "2025-08-06T23:09:11.6208684Z",
       "session_id": "da73cb9d-a63b-4402-8289-ae75c02491ac",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 8,
       "statement_ids": [
        8
       ]
      },
      "text/plain": [
       "StatementMeta(, da73cb9d-a63b-4402-8289-ae75c02491ac, 8, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-22 22:00:00\n",
      "2025-05-16 06:00:00\n"
     ]
    }
   ],
   "source": [
    "print(weather_forecast_df['Datetime'].max())\n",
    "print(weather_forecast_df['Datetime'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf61bdd-c701-4c66-bb98-6d20669272fb",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-06T23:09:59.5810414Z",
       "execution_start_time": "2025-08-06T23:09:58.6648544Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "35462d06-ccfd-4da5-bd7c-39c5aa7cab3d",
       "queued_time": "2025-08-06T23:09:11.6245042Z",
       "session_id": "da73cb9d-a63b-4402-8289-ae75c02491ac",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 9,
       "statement_ids": [
        9
       ]
      },
      "text/plain": [
       "StatementMeta(, da73cb9d-a63b-4402-8289-ae75c02491ac, 9, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Temp</th>\n",
       "      <th>IsRaining</th>\n",
       "      <th>IsSnowing</th>\n",
       "      <th>Hour</th>\n",
       "      <th>ParkName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-05-16 06:00:00</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>sθәqәlxenәm ts'exwts'áxwi7 (Rainbow)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-05-16 06:00:00</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>Harbour Green Park</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-05-16 06:00:00</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>Guelph Park</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-05-16 06:00:00</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>Grandview Park</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-05-16 06:00:00</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>General Brock Park</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Datetime  Temp  IsRaining  IsSnowing  Hour  \\\n",
       "0 2025-05-16 06:00:00   8.6          1          0     6   \n",
       "1 2025-05-16 06:00:00   8.6          1          0     6   \n",
       "2 2025-05-16 06:00:00   8.6          1          0     6   \n",
       "3 2025-05-16 06:00:00   8.6          1          0     6   \n",
       "4 2025-05-16 06:00:00   8.6          1          0     6   \n",
       "\n",
       "                                ParkName  \n",
       "0  sθәqәlxenәm ts'exwts'áxwi7 (Rainbow)  \n",
       "1                     Harbour Green Park  \n",
       "2                            Guelph Park  \n",
       "3                         Grandview Park  \n",
       "4                     General Brock Park  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Directory where models are stored\n",
    "model_dir = \"/lakehouse/default/Files/models/live_times\"\n",
    "\n",
    "# Discover which parks you have models for ───\n",
    "model_files = [fn for fn in os.listdir(model_dir) if fn.endswith(\".joblib\")]\n",
    "trained_parks = [\n",
    "    os.path.splitext(fn)[0].replace(\"_\", \" \")\n",
    "    for fn in model_files\n",
    "]\n",
    "\n",
    "# Create metadata dataframe\n",
    "park_meta_df = pd.DataFrame({\"ParkName\": trained_parks})\n",
    "park_meta_df[\"key\"] = 1\n",
    "weather_forecast_df[\"key\"] = 1\n",
    "\n",
    "# Cross join\n",
    "forecast_df = pd.merge(weather_forecast_df, park_meta_df, on=\"key\").drop(columns=\"key\")\n",
    "forecast_df = forecast_df.sort_values(by=\"Datetime\").reset_index(drop=True)\n",
    "forecast_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da06a14c-b24b-4494-aab8-db240acbbdf7",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Join with Park attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eba86d4-31d8-4c56-8d0e-8def62d5f167",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-06T23:10:04.4586843Z",
       "execution_start_time": "2025-08-06T23:09:59.5837193Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "41771c92-aee4-4fbb-bad1-7c51f9f1276c",
       "queued_time": "2025-08-06T23:09:11.6280494Z",
       "session_id": "da73cb9d-a63b-4402-8289-ae75c02491ac",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 10,
       "statement_ids": [
        10
       ]
      },
      "text/plain": [
       "StatementMeta(, da73cb9d-a63b-4402-8289-ae75c02491ac, 10, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Join forecast_df with dim_park_attributes on ParkName\n",
    "dim_park_attributes = (\n",
    "    spark.read.table(\"dim_park_attributes\")\n",
    "    .select(\"ParkName\", \"ParkKey\",\"PlaceID\")\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "# Join to get ParkKey into forecast_df\n",
    "forecast_df = forecast_df.merge(dim_park_attributes, on=\"ParkName\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc70226-c918-486c-9bdf-66b946a9cc78",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Join with dim_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ef8d52-21fd-49fb-a295-86b751b34449",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-06T23:10:08.0511042Z",
       "execution_start_time": "2025-08-06T23:10:04.4613516Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "d3b9a0e2-4144-444e-bee2-40cf68ef22e2",
       "queued_time": "2025-08-06T23:09:11.6318644Z",
       "session_id": "da73cb9d-a63b-4402-8289-ae75c02491ac",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 11,
       "statement_ids": [
        11
       ]
      },
      "text/plain": [
       "StatementMeta(, da73cb9d-a63b-4402-8289-ae75c02491ac, 11, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Join forecast_df with dim_date  \n",
    "forecast_df['Date'] = forecast_df['Datetime'].dt.date  \n",
    "\n",
    "dim_date = (\n",
    "    spark.read.table(\"dim_date\")\n",
    "    .select(\"Date\", \"DayOfWeek\", \"IsWeekend\", \"IsHoliday\")\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "dim_date['Date'] = pd.to_datetime(dim_date['Date']).dt.date\n",
    "\n",
    "forecast_df = forecast_df.merge(dim_date, on=\"Date\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7c87eb-87be-402a-bb8b-b30c2193e072",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Join with dim_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a2b2c9-1c1f-4556-9009-cd9b4dac046c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-06T23:10:13.0891138Z",
       "execution_start_time": "2025-08-06T23:10:08.0543536Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "cb9957c9-b584-4a9f-bc71-25e9b1e10645",
       "queued_time": "2025-08-06T23:09:11.6355553Z",
       "session_id": "da73cb9d-a63b-4402-8289-ae75c02491ac",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 12,
       "statement_ids": [
        12
       ]
      },
      "text/plain": [
       "StatementMeta(, da73cb9d-a63b-4402-8289-ae75c02491ac, 12, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Join forecast_df with dim_event\n",
    "dim_events = (\n",
    "    spark.read.table(\"dim_events\")\n",
    "    .select(\"EventKey\", \"PlaceID\", \"StartTime\", \"EndTime\")\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "dim_events['StartTime'] = pd.to_datetime(dim_events['StartTime'])\n",
    "dim_events['EndTime'] = pd.to_datetime(dim_events['EndTime'])\n",
    "\n",
    "# Step 1: Inner join with events where Datetime is within Start–End window\n",
    "matched = forecast_df.merge(dim_events, on=\"PlaceID\", how=\"left\")\n",
    "matched = matched[\n",
    "    (matched['Datetime'] >= matched['StartTime']) &\n",
    "    (matched['Datetime'] <= matched['EndTime'])\n",
    "]\n",
    "\n",
    "# Step 2: Create a set of rows that had an event\n",
    "matched_set = matched[['Datetime', 'ParkName', 'PlaceID']].drop_duplicates()\n",
    "matched_set['HasEvent'] = 1\n",
    "\n",
    "# Step 3: Merge HasEvent back into forecast_df (default = 0 if no match)\n",
    "forecast_df = forecast_df.merge(\n",
    "    matched_set,\n",
    "    on=['Datetime', 'ParkName', 'PlaceID'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "forecast_df['HasEvent'] = forecast_df['HasEvent'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2d719a-5519-4a0d-9e0d-f3d49e33a654",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### **Feature Engineering Forecast-Ready Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f55400c-5e00-4480-a207-7592ca63b1d2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-06T23:10:13.4603818Z",
       "execution_start_time": "2025-08-06T23:10:13.0919537Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "f60911d0-4f03-4345-897d-f50a6e11d1ae",
       "queued_time": "2025-08-06T23:09:11.6390822Z",
       "session_id": "da73cb9d-a63b-4402-8289-ae75c02491ac",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 13,
       "statement_ids": [
        13
       ]
      },
      "text/plain": [
       "StatementMeta(, da73cb9d-a63b-4402-8289-ae75c02491ac, 13, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7378 entries, 0 to 7377\n",
      "Data columns (total 13 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   Datetime   7378 non-null   datetime64[us]\n",
      " 1   Temp       7378 non-null   float64       \n",
      " 2   IsRaining  7378 non-null   int32         \n",
      " 3   IsSnowing  7378 non-null   int32         \n",
      " 4   Hour       7378 non-null   int32         \n",
      " 5   ParkName   7378 non-null   object        \n",
      " 6   ParkKey    7378 non-null   int64         \n",
      " 7   PlaceID    7378 non-null   object        \n",
      " 8   Date       7378 non-null   object        \n",
      " 9   DayOfWeek  7378 non-null   int64         \n",
      " 10  IsWeekend  7378 non-null   int64         \n",
      " 11  IsHoliday  7378 non-null   int32         \n",
      " 12  HasEvent   7378 non-null   int64         \n",
      "dtypes: datetime64[us](1), float64(1), int32(4), int64(4), object(3)\n",
      "memory usage: 634.2+ KB\n"
     ]
    }
   ],
   "source": [
    "forecast_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94464f77-5edc-44ae-9709-ecf277800165",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-06T23:10:13.9678223Z",
       "execution_start_time": "2025-08-06T23:10:13.4626574Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "acf4274f-f7fd-48bd-9f84-1d265aa53e55",
       "queued_time": "2025-08-06T23:09:11.6425133Z",
       "session_id": "da73cb9d-a63b-4402-8289-ae75c02491ac",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 14,
       "statement_ids": [
        14
       ]
      },
      "text/plain": [
       "StatementMeta(, da73cb9d-a63b-4402-8289-ae75c02491ac, 14, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Timestamp('2025-05-22 22:00:00')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_df['Datetime'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cad4aea-66be-48ae-9550-f36d4d399d59",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-06T23:10:40.465375Z",
       "execution_start_time": "2025-08-06T23:10:13.970446Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "c94598b1-3c06-45fd-9181-3e4d0316d3f9",
       "queued_time": "2025-08-06T23:09:11.6460882Z",
       "session_id": "da73cb9d-a63b-4402-8289-ae75c02491ac",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 15,
       "statement_ids": [
        15
       ]
      },
      "text/plain": [
       "StatementMeta(, da73cb9d-a63b-4402-8289-ae75c02491ac, 15, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stored 62 parks | Expected rows: 119\n",
      "\n",
      " Unusual parks:\n",
      " - Clinton Park: 118 rows (expected 119) → 1 missing rows\n",
      " - Hastings Park - Italian Garden: 0 rows (expected 119) → no data\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from datetime import timedelta\n",
    "\n",
    "# Set forecast start and 7-day window\n",
    "forecast_df = forecast_df.set_index('Datetime')  \n",
    "forecast_start = forecast_df.index.min().normalize()\n",
    "start_window, end_window = forecast_start - timedelta(days=7), forecast_start\n",
    "\n",
    "# Expected rows: number of hours × number of days in window\n",
    "window_df = imputed_fact_live_times.loc[(imputed_fact_live_times.index >= start_window) & \n",
    "                                        (imputed_fact_live_times.index < end_window)]\n",
    "expected_rows = window_df.index.hour.nunique() * window_df.index.normalize().nunique()\n",
    "\n",
    "# Build park_last_known_dict using loaded models\n",
    "park_last_known_dict = {}\n",
    "warnings = []\n",
    "\n",
    "for park_name in trained_parks:\n",
    "    # Construct model path and load model\n",
    "    filename = park_name.replace(\" \", \"_\").replace(\"/\", \"_\") + \".joblib\"\n",
    "    model_path = os.path.join(model_dir, filename)\n",
    "    \n",
    "    try:\n",
    "        model = joblib.load(model_path)\n",
    "    except Exception as e:\n",
    "        warnings.append(f\"{park_name}: failed to load model ({e})\")\n",
    "        continue\n",
    "\n",
    "    # Get last 7-day historical data for this park\n",
    "    df_park = imputed_fact_live_times[imputed_fact_live_times['ParkName'] == park_name].copy()\n",
    "    last_7day = df_park[(df_park.index >= start_window) & (df_park.index < end_window)].sort_index()\n",
    "\n",
    "    # Store in dictionary\n",
    "    park_last_known_dict[park_name] = {\n",
    "        'model': model,\n",
    "        'last_known': last_7day,\n",
    "    }\n",
    "\n",
    "    # Check row completeness\n",
    "    n = len(last_7day)\n",
    "    if n != expected_rows:\n",
    "        note = \"no data\" if n == 0 else f\"{abs(n - expected_rows)} {'missing' if n < expected_rows else 'extra'} rows\"\n",
    "        warnings.append(f\"{park_name}: {n} rows (expected {expected_rows}) → {note}\")\n",
    "\n",
    "# Summary output\n",
    "print(f\"\\nStored {len(park_last_known_dict)} parks | Expected rows: {expected_rows}\")\n",
    "if warnings:\n",
    "    print(\"\\n Unusual parks:\\n - \" + \"\\n - \".join(warnings))\n",
    "else:\n",
    "    print(\"All parks have complete 7-day historical data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4d9a30-cdbb-43bf-87d4-3fad236bc2e0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-06T23:10:41.4130645Z",
       "execution_start_time": "2025-08-06T23:10:40.4674382Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "b83f0908-f0ff-4457-b849-8f41a3e9aacd",
       "queued_time": "2025-08-06T23:09:11.6503308Z",
       "session_id": "da73cb9d-a63b-4402-8289-ae75c02491ac",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 16,
       "statement_ids": [
        16
       ]
      },
      "text/plain": [
       "StatementMeta(, da73cb9d-a63b-4402-8289-ae75c02491ac, 16, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andy Livingstone Park: 238 total rows\n",
      "Arbutus Greenway Park: 238 total rows\n",
      "Beaconsfield Park: 238 total rows\n",
      "Bobolink Park: 238 total rows\n",
      "CRAB Park at Portside: 238 total rows\n",
      "Chaldecott Park: 238 total rows\n",
      "Champlain Heights Park: 238 total rows\n",
      "Charleson Park: 238 total rows\n",
      "China Creek North Park: 238 total rows\n",
      "China Creek South Park: 238 total rows\n",
      "Clark Park: 238 total rows\n",
      "Clinton Park: 237 total rows\n",
      "Coal Harbour Park: 238 total rows\n",
      "Columbia Park: 238 total rows\n",
      "Connaught Park: 238 total rows\n",
      "Coopers' Park: 238 total rows\n",
      "Creekside Park: 238 total rows\n",
      "David Lam Park: 238 total rows\n",
      "Emery Barnes Park: 238 total rows\n",
      "Empire Fields - Hastings Park: 238 total rows\n",
      "English Bay Beach Park: 238 total rows\n",
      "Everett Crowley Park: 238 total rows\n",
      "Falaise Park: 238 total rows\n",
      "Fraser River Park: 238 total rows\n",
      "General Brock Park: 238 total rows\n",
      "Grandview Park: 238 total rows\n",
      "Guelph Park: 238 total rows\n",
      "Harbour Green Park: 238 total rows\n",
      "Hastings Park - Italian Garden: 119 total rows\n",
      "Hillcrest Park: 238 total rows\n",
      "Jericho Beach Park: 238 total rows\n",
      "John Hendry (Trout Lake) Park: 238 total rows\n",
      "Jonathan Rogers Park: 238 total rows\n",
      "Kaslo Park: 238 total rows\n",
      "Kitsilano Beach Park: 238 total rows\n",
      "Memorial South Park: 238 total rows\n",
      "Memorial West Park: 238 total rows\n",
      "Mount Pleasant Park: 238 total rows\n",
      "Musqueam Park: 238 total rows\n",
      "Nelson Park: 238 total rows\n",
      "New Brighton Park: 238 total rows\n",
      "Oak Meadows Park: 238 total rows\n",
      "Oak Park: 238 total rows\n",
      "Oppenheimer Park: 238 total rows\n",
      "Pandora Park: 238 total rows\n",
      "Queen Elizabeth Park: 238 total rows\n",
      "Quilchena Park: 238 total rows\n",
      "Riverfront Park: 238 total rows\n",
      "Robson Park: 238 total rows\n",
      "Rupert Park: 238 total rows\n",
      "Slocan Park: 238 total rows\n",
      "Spanish Banks Beach Park: 238 total rows\n",
      "Stanley Park: 238 total rows\n",
      "Strathcona Park: 238 total rows\n",
      "Sunrise Park: 238 total rows\n",
      "Sunset Beach Park: 238 total rows\n",
      "Tatlow Park: 238 total rows\n",
      "Trillium Park: 238 total rows\n",
      "Vandusen Botanical Garden: 238 total rows\n",
      "Vanier Park: 238 total rows\n",
      "Woodland Park: 238 total rows\n",
      "sθәqәlxenәm ts'exwts'áxwi7 (Rainbow): 238 total rows\n",
      "\n",
      " Ready for prediction: 62 parks\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import pi\n",
    "\n",
    "target_col = \"PopularTimesLivePercent\"\n",
    "park_col = \"ParkName\"\n",
    "\n",
    "merged_forecast_dict = {}\n",
    "\n",
    "for park in park_last_known_dict.keys():\n",
    "    # Get last 7-day historical data\n",
    "    last_known = park_last_known_dict[park]['last_known'].copy()\n",
    "\n",
    "    # Get forecast data for this park\n",
    "    future_input = forecast_df[forecast_df[park_col] == park].copy()\n",
    "\n",
    "    # Ensure index is datetime\n",
    "    future_input.index = pd.to_datetime(future_input.index)\n",
    "    \n",
    "    # Insert placeholder target column\n",
    "    future_input[target_col] = np.nan\n",
    "\n",
    "    # Combine historical + future\n",
    "    combined = pd.concat([last_known, future_input]).sort_index()\n",
    "    combined.index = pd.to_datetime(combined.index)\n",
    "\n",
    "    # Identify forecast portion\n",
    "    forecast_mask = combined[target_col].isna()\n",
    "\n",
    "    # Feature engineering (only on forecast rows)\n",
    "    combined.loc[forecast_mask, 'Hour'] = combined.loc[forecast_mask].index.hour\n",
    "    combined.loc[forecast_mask, 'HourOp'] = (combined.loc[forecast_mask, 'Hour'] - 6) % 17\n",
    "    combined.loc[forecast_mask, 'DayOfWeek'] = combined.loc[forecast_mask].index.dayofweek\n",
    "    combined.loc[forecast_mask, 'HourSin'] = np.sin(2 * pi * combined.loc[forecast_mask, 'HourOp'] / 17)\n",
    "    combined.loc[forecast_mask, 'HourCos'] = np.cos(2 * pi * combined.loc[forecast_mask, 'HourOp'] / 17)\n",
    "\n",
    "    # Create lag features\n",
    "    for lag in [1, 2, 3, 17]:\n",
    "        combined[f'Lag_{lag}'] = combined[target_col].shift(lag)\n",
    "\n",
    "    # Handle unreliable lag at 6AM using 6AM averages by day of week\n",
    "    historical_data = combined[~forecast_mask].copy()\n",
    "    historical_data['DayOfWeek'] = historical_data.index.dayofweek\n",
    "\n",
    "    avg_6am_by_dow = (\n",
    "        historical_data[historical_data.index.hour == 6]\n",
    "        .groupby('DayOfWeek')[target_col]\n",
    "        .mean()\n",
    "    )\n",
    "\n",
    "    forecast_6am_mask = forecast_mask & (combined.index.hour == 6)\n",
    "    dow_values = combined.loc[forecast_6am_mask].index.dayofweek.to_series(index=combined.loc[forecast_6am_mask].index)\n",
    "\n",
    "    for lag in [1, 2, 3, 17]:\n",
    "        combined.loc[forecast_6am_mask, f'Lag_{lag}'] = dow_values.map(avg_6am_by_dow)\n",
    "\n",
    "    # Store result\n",
    "    merged_forecast_dict[park] = combined\n",
    "    print(f\"{park}: {len(combined)} total rows\")\n",
    "\n",
    "print(f\"\\n Ready for prediction: {len(merged_forecast_dict)} parks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e956591-d4fc-4cf3-bb72-53e2be8edb51",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24eddbf0-38c2-479f-9f5f-2bf958ef9892",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Prediction Pipeline - GitHub PlotURL \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import os\n",
    "import base64\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "def create_forecast(merged_forecast_dict, park_last_known_dict, save_to_lakehouse=True):\n",
    "    \"\"\"\n",
    "    Create clean and elegant forecast visualization with 17-hour operational granularity (6am-10pm)\n",
    "    Includes auto-save functionality to lakehouse\n",
    "    \"\"\"\n",
    "    \n",
    "    # AUTO-SAVE SETUP\n",
    "    if save_to_lakehouse:\n",
    "        today_str = datetime.now().strftime(\"%Y%m%d\")\n",
    "        save_dir = f\"/lakehouse/default/Files/forecast/{today_str}\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        print(f\"Auto-save enabled: {save_dir}\")\n",
    "    \n",
    "    # Define the 15 features your model expects\n",
    "    feature_cols = [\n",
    "        'Temp', 'IsRaining', 'IsSnowing', 'IsHoliday', 'IsWeekend', 'HasEvent',\n",
    "        'HourOp', 'DayOfWeek', 'HourSin', 'HourCos', \n",
    "        'Lag_1', 'Lag_2', 'Lag_3', 'Lag_17',\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # STEP 1: Generate Predictions\n",
    "    print(\"Generating predictions...\")\n",
    "    \n",
    "    for park_name in merged_forecast_dict.keys():\n",
    "        df = merged_forecast_dict[park_name].copy()\n",
    "        model = park_last_known_dict[park_name]['model']\n",
    "        \n",
    "        future_mask = df['PopularTimesLivePercent'].isna()\n",
    "        future_indices = df[future_mask].index.sort_values()\n",
    "        \n",
    "        # Make predictions iteratively\n",
    "        for i, idx in enumerate(future_indices):\n",
    "            #X = df.loc[idx:idx, feature_cols]\n",
    "            X = df.loc[[idx], feature_cols]\n",
    "            prediction = np.clip(model.predict(X)[0], 0, 100)\n",
    "            df.loc[idx, 'PopularTimesLivePercent'] = prediction\n",
    "            \n",
    "            # Update lag features\n",
    "            remaining_indices = future_indices[i+1:]\n",
    "            for j, next_idx in enumerate(remaining_indices):\n",
    "                if next_idx not in df.index:\n",
    "                    continue\n",
    "                if j == 0:\n",
    "                    df.loc[next_idx, 'Lag_1'] = prediction\n",
    "                elif j == 1:\n",
    "                    lag2 = df.loc[future_indices[i-1], 'PopularTimesLivePercent'] if i > 0 else prediction\n",
    "                    df.loc[next_idx, 'Lag_2'] = np.clip(lag2, 0, 100)\n",
    "                elif j == 2:\n",
    "                    lag3 = df.loc[future_indices[i-2], 'PopularTimesLivePercent'] if i > 1 else prediction\n",
    "                    df.loc[next_idx, 'Lag_3'] = np.clip(lag3, 0, 100)\n",
    "                elif j == 16:\n",
    "                    lag17 = df.loc[future_indices[i-16], 'PopularTimesLivePercent'] if i >= 16 else prediction\n",
    "                    df.loc[next_idx, 'Lag_17'] = np.clip(lag17, 0, 100)\n",
    "\n",
    "        results[park_name] = {'data': df, 'future_mask': future_mask}\n",
    "        \n",
    "    \n",
    "    # STEP 2: Create Clean Visualization for 17-Hour Operations\n",
    "    print(\" Creating 17-hour operational visualization...\")\n",
    "    \n",
    "    # Set up modern, clean style\n",
    "    plt.style.use('default')\n",
    "    \n",
    "    # Modern color palette\n",
    "    colors = {\n",
    "        'historical': '#0279b1',    # Deep blue\n",
    "        'forecast': '#4b8516',      # Green\n",
    "        'transition': '#444444',    # Gray\n",
    "        'background': '#FAFAFA',    # Light gray\n",
    "        'grid': '#E0E0E0'          # Subtle gray\n",
    "    }\n",
    "    \n",
    "    # CREATE INDIVIDUAL PLOTS FOR EACH PARK \n",
    "    for park_name, result in results.items():\n",
    "        # Create individual figure for each park\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(18, 6))\n",
    "        \n",
    "        df = result['data']\n",
    "        future_mask = result['future_mask']\n",
    "        \n",
    "        # Split data\n",
    "        historical = df[~future_mask]\n",
    "        predicted = df[future_mask]\n",
    "        \n",
    "        # Clean background\n",
    "        ax.set_facecolor(colors['background'])\n",
    "        fig.patch.set_facecolor('white')\n",
    "        \n",
    "        # Plot historical data\n",
    "        if len(historical) > 0:\n",
    "            # Main historical line\n",
    "            ax.plot(historical.index, historical['PopularTimesLivePercent'], \n",
    "                   color=colors['historical'], linewidth=2.5, \n",
    "                   label='Historical Data', alpha=0.9, zorder=3)\n",
    "            \n",
    "            # Add hourly markers for historical data\n",
    "            ax.scatter(historical.index, historical['PopularTimesLivePercent'], \n",
    "                      color=colors['historical'], s=12, alpha=0.6, zorder=4)\n",
    "        \n",
    "        # Plot forecast with simplified confidence analysis\n",
    "        if len(predicted) > 0:\n",
    "            # Main forecast line\n",
    "            ax.plot(predicted.index, predicted['PopularTimesLivePercent'], \n",
    "                   color=colors['forecast'], linewidth=2.5, \n",
    "                   label='7-Day Forecast', alpha=0.9, zorder=3, linestyle='--')\n",
    "            \n",
    "            # Add hourly markers for forecast\n",
    "            ax.scatter(predicted.index, predicted['PopularTimesLivePercent'],\n",
    "                      color=colors['forecast'], s=12, alpha=0.6, \n",
    "                      marker='s', zorder=4)\n",
    "            \n",
    "            # Simplified confidence analysis\n",
    "            pred_values = predicted['PopularTimesLivePercent'].values\n",
    "            \n",
    "            # Calculate overall confidence based on:\n",
    "            # 1. Prediction stability (low variance = high confidence)\n",
    "            # 2. Similarity to historical patterns\n",
    "            # 3. Trend smoothness\n",
    "            \n",
    "            # Method 1: Rolling standard deviation for temporal consistency\n",
    "            rolling_window = max(1, min(12, len(pred_values) // 3))\n",
    "            rolling_std = (pd.Series(pred_values).rolling(window=rolling_window, center=True, min_periods=1).std())\n",
    "            rolling_std = rolling_std.fillna(rolling_std.mean())\n",
    "            \n",
    "            # Method 2: Distance from historical mean for stability\n",
    "            if len(historical) > 0:\n",
    "                hist_mean = historical['PopularTimesLivePercent'].mean()\n",
    "                deviation_from_hist = np.abs(pred_values - hist_mean)\n",
    "                stability_factor = 1 - np.minimum(deviation_from_hist / 50, 1)\n",
    "            else:\n",
    "                stability_factor = np.ones(len(pred_values)) * 0.7\n",
    "            \n",
    "            # Method 3: Trend smoothness\n",
    "            if len(pred_values) > 1:\n",
    "                pred_diff = np.abs(np.diff(pred_values))\n",
    "                smoothness_factor = 1 - np.minimum(pred_diff / 20, 1)\n",
    "                smoothness_factor = np.concatenate([[smoothness_factor[0]], smoothness_factor])\n",
    "            else:\n",
    "                smoothness_factor = np.ones(len(pred_values))\n",
    "            \n",
    "            # Combined overall confidence (0-100%)\n",
    "            variance_conf  = 100 * (1 - np.minimum(rolling_std / 15, 1))\n",
    "            stability_conf = 100 * stability_factor\n",
    "            smoothness_conf= 100 * smoothness_factor\n",
    "            overall_conf   = 0.4*variance_conf + 0.4*stability_conf + 0.2*smoothness_conf\n",
    "            avg_confidence = np.mean(overall_conf)\n",
    "            \n",
    "            # R² based adjustment\n",
    "            r2_score = park_last_known_dict.get(park_name, {}).get('metrics', {}).get('R2', 0)\n",
    "            if r2_score < 0.0:\n",
    "                avg_confidence *= 0.4\n",
    "            elif r2_score < 0.20:\n",
    "                avg_confidence *= 0.6\n",
    "            elif r2_score < 0.40:\n",
    "                avg_confidence *= 0.8\n",
    "            \n",
    "            # Single confidence interval based on overall confidence\n",
    "            confidence_factor = avg_confidence / 100\n",
    "            adaptive_std = rolling_std * (2.5 - 1.5 * confidence_factor)  # Higher confidence = narrower bands\n",
    "            \n",
    "            upper_band = pred_values + adaptive_std\n",
    "            lower_band = np.maximum(pred_values - adaptive_std, 0)\n",
    "            \n",
    "            # Single confidence band\n",
    "            ax.fill_between(predicted.index, lower_band, upper_band, \n",
    "                           color=colors['forecast'], alpha=0.2, \n",
    "                           label=f'Confidence Interval ({avg_confidence:.0f}%)', zorder=1)\n",
    "            \n",
    "            # Store simplified confidence metrics\n",
    "            result['confidence_metrics'] = {\n",
    "                'avg_confidence': avg_confidence,\n",
    "                'min_confidence': np.min(overall_conf),\n",
    "                'max_confidence': np.max(overall_conf),\n",
    "                'confidence_values': overall_conf\n",
    "            }\n",
    "        \n",
    "        # Connect historical to forecast\n",
    "        if len(historical) > 0 and len(predicted) > 0:\n",
    "            transition_x = [historical.index[-1], predicted.index[0]]\n",
    "            transition_y = [historical['PopularTimesLivePercent'].iloc[-1], \n",
    "                           predicted['PopularTimesLivePercent'].iloc[0]]\n",
    "            \n",
    "            ax.plot(transition_x, transition_y, \n",
    "                   color=colors['transition'], linewidth=2.5, \n",
    "                   alpha=0.8, zorder=3)\n",
    "        \n",
    "        # OPERATIONAL HOURS X-AXIS FORMATTING (6am-10pm only)\n",
    "        data_start = df.index.min()\n",
    "        data_end = df.index.max()\n",
    "        \n",
    "        # Create custom tick locations for operational hours\n",
    "        major_times = []\n",
    "        all_times = []\n",
    "        \n",
    "        current_date = data_start.normalize()\n",
    "        end_date = data_end.normalize() + pd.Timedelta(days=1)\n",
    "        \n",
    "        while current_date <= end_date:\n",
    "            for hour in range(6, 23):  # 6am to 10pm (17 hours)\n",
    "                timestamp = current_date.replace(hour=hour)\n",
    "                if data_start <= timestamp <= data_end:\n",
    "                    all_times.append(timestamp)\n",
    "                    # Major ticks every 4 hours during operations (6am, 10am, 2pm, 6pm, 10pm)\n",
    "                    if hour in [6, 10, 14, 18, 22]:\n",
    "                        major_times.append(timestamp)\n",
    "            current_date += pd.Timedelta(days=1)\n",
    "        \n",
    "        # Set custom ticks\n",
    "        ax.set_xticks(major_times)\n",
    "        ax.set_xticks(all_times, minor=True)\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "        \n",
    "        # Rotate labels for better readability\n",
    "        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right', fontsize=10,color='black')\n",
    "        \n",
    "        # Add secondary axis for operational days with day names\n",
    "        ax2 = ax.twiny()\n",
    "        ax2.set_xlim(ax.get_xlim())\n",
    "        \n",
    "        # Position day labels at operational midpoint (1pm)\n",
    "        day_positions = []\n",
    "        day_labels = []\n",
    "        current_date = data_start.normalize()\n",
    "        \n",
    "        while current_date <= data_end.normalize():\n",
    "            midpoint = current_date.replace(hour=13)  # 1pm - middle of operational day\n",
    "            if data_start <= midpoint <= data_end:\n",
    "                day_positions.append(midpoint)\n",
    "                day_labels.append(midpoint.strftime('%a, %b %d'))  # Mon, May 16\n",
    "            current_date += pd.Timedelta(days=1)\n",
    "        \n",
    "        ax2.set_xticks(day_positions)\n",
    "        ax2.set_xticklabels(day_labels)\n",
    "        ax2.tick_params(axis='x', labelsize=11, colors='black', length=4, pad=5)\n",
    "        ax2.spines['top'].set_visible(False)\n",
    "        ax2.spines['bottom'].set_visible(False)\n",
    "        ax2.spines['left'].set_visible(False)\n",
    "        ax2.spines['right'].set_visible(False)\n",
    "        \n",
    "        # Add operational day markers\n",
    "        current_date = data_start.normalize()\n",
    "        while current_date <= data_end.normalize():\n",
    "            # Start of operational day (6am) - Green line\n",
    "            day_start = current_date.replace(hour=6)\n",
    "            if data_start <= day_start <= data_end:\n",
    "                ax.axvline(x=day_start, color='#4CAF50', linestyle='-', alpha=0.6, linewidth=1.5, zorder=0)\n",
    "                \n",
    "            # End of operational day (10pm) - Orange line\n",
    "            day_end = current_date.replace(hour=22)\n",
    "            if data_start <= day_end <= data_end:\n",
    "                ax.axvline(x=day_end, color='#FF9800', linestyle='-', alpha=0.6, linewidth=1.5, zorder=0)\n",
    "            \n",
    "            # Midday marker (1pm) - Subtle dashed line\n",
    "            midday = current_date.replace(hour=13)\n",
    "            if data_start <= midday <= data_end:\n",
    "                ax.axvline(x=midday, color='#E0E0E0', linestyle='--', alpha=0.4, linewidth=0.8, zorder=0)\n",
    "                \n",
    "            current_date += pd.Timedelta(days=1)\n",
    "        \n",
    "        # Highlight weekends with subtle background shading\n",
    "        current_date = data_start.normalize()\n",
    "        while current_date <= data_end.normalize():\n",
    "            if current_date.weekday() >= 5:  # Saturday = 5, Sunday = 6\n",
    "                weekend_start = max(current_date.replace(hour=6), data_start)\n",
    "                weekend_end = min(current_date.replace(hour=22), data_end)\n",
    "                if weekend_start <= data_end and weekend_end >= data_start:\n",
    "                    ax.axvspan(weekend_start, weekend_end, alpha=0.05, color='#2196F3', zorder=0)\n",
    "            current_date += pd.Timedelta(days=1)\n",
    "        \n",
    "        # Title and labels\n",
    "        ax.set_title(f'{park_name} - Operational Hours Forecast (6AM-10PM)', \n",
    "                    fontsize=18, fontweight='400', color='#2C3E50', pad=20)\n",
    "        \n",
    "        ax.set_ylabel('Occupancy (%)', \n",
    "                     fontsize=12, color='black', fontweight='400')\n",
    "        \n",
    "        # Clean axis styling\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['left'].set_color('#BDC3C7')\n",
    "        ax.spines['bottom'].set_color('#BDC3C7')\n",
    "        \n",
    "        # Enhanced grid for operational hours visibility\n",
    "        ax.grid(True, which='major', alpha=0.4, color=colors['grid'], linewidth=0.8)\n",
    "        ax.grid(True, which='minor', alpha=0.2, color=colors['grid'], linewidth=0.5)\n",
    "        ax.set_axisbelow(True)\n",
    "        \n",
    "        # Elegant tick styling\n",
    "        ax.tick_params(axis='both', which='major', \n",
    "                      labelsize=10, colors='black', \n",
    "                      length=5, width=1)\n",
    "        ax.tick_params(axis='both', which='minor', \n",
    "                      length=3, width=0.5, colors='black')\n",
    "        \n",
    "        # Set reasonable y-limits\n",
    "        all_values = df['PopularTimesLivePercent'].dropna()\n",
    "        if len(all_values) > 0:\n",
    "            y_min = max(0, all_values.min() - 5)\n",
    "            y_max = min(100, all_values.max() + 10)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "        \n",
    "        # Modern legend\n",
    "        legend = ax.legend(loc='upper right', frameon=True, \n",
    "                          fancybox=True, shadow=False, \n",
    "                          fontsize=11, facecolor='white', \n",
    "                          edgecolor='#BDC3C7', framealpha=0.9)\n",
    "        \n",
    "        # Add confidence insights\n",
    "        if len(predicted) > 0:\n",
    "            pred_avg = predicted['PopularTimesLivePercent'].mean()\n",
    "            hist_avg = historical['PopularTimesLivePercent'].mean() if len(historical) > 0 else 0\n",
    "            \n",
    "            # Find peak hour within operational hours\n",
    "            pred_hourly = predicted.groupby(predicted.index.hour)['PopularTimesLivePercent'].mean()\n",
    "            peak_hour = pred_hourly.idxmax()\n",
    "            \n",
    "            # Get confidence metrics\n",
    "            conf_metrics = result.get('confidence_metrics', {})\n",
    "            avg_conf = conf_metrics.get('avg_confidence', 0)\n",
    "            \n",
    "            # Main info box with confidence\n",
    "            info_text = f'Operational Avg: {pred_avg:.0f}% | Peak: {peak_hour}:00\\nAvg Confidence: {avg_conf:.0f}%'\n",
    "            ax.text(0.02, 0.98, info_text, \n",
    "                   transform=ax.transAxes, \n",
    "                   bbox=dict(boxstyle=\"round,pad=0.5\", \n",
    "                            facecolor='white', \n",
    "                            edgecolor='#BDC3C7',\n",
    "                            alpha=0.9),\n",
    "                   fontsize=10, color='black',\n",
    "                   verticalalignment='top')\n",
    "            \n",
    "            # Confidence breakdown box\n",
    "            min_conf = conf_metrics.get('min_confidence', 0)\n",
    "            max_conf = conf_metrics.get('max_confidence', 0)\n",
    "            conf_text = f'Confidence Range: {min_conf:.0f}% - {max_conf:.0f}%'\n",
    "            \n",
    "            ax.text(0.98, 0.98, conf_text, \n",
    "                   transform=ax.transAxes, \n",
    "                   bbox=dict(boxstyle=\"round,pad=0.3\", \n",
    "                            facecolor='#F0F8FF', \n",
    "                            edgecolor='#87CEEB',\n",
    "                            alpha=0.9),\n",
    "                   fontsize=9, color='black',\n",
    "                   verticalalignment='top', horizontalalignment='right')\n",
    "            \n",
    "            # Operational hours detail annotation\n",
    "            total_hours = len(df)\n",
    "            hist_hours = len(historical)\n",
    "            pred_hours = len(predicted)\n",
    "            \n",
    "            detail_text = f'Operating Hours: 6AM-10PM (17h) | {hist_hours}h Historical | {pred_hours}h Forecast'\n",
    "            ax.text(0.02, 0.02, detail_text, \n",
    "                   transform=ax.transAxes, \n",
    "                   bbox=dict(boxstyle=\"round,pad=0.3\", \n",
    "                            facecolor='#F8F9FA', \n",
    "                            edgecolor='#DEE2E6',\n",
    "                            alpha=0.9),\n",
    "                   fontsize=9, color='black',\n",
    "                   verticalalignment='bottom')\n",
    "        \n",
    "        # Final layout for individual plot\n",
    "        plt.tight_layout(pad=2.0)\n",
    "        \n",
    "        # AUTO-SAVE INDIVIDUAL PLOT \n",
    "        if save_to_lakehouse:\n",
    "            safe_name = park_name.replace(\"/\", \"-\").replace(\" \", \"_\").replace(\",\", \"\") + \".png\"\n",
    "            fig_path = os.path.join(save_dir, safe_name)\n",
    "            plt.savefig(fig_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "        \n",
    "        #Github code will be here.\n",
    "        # === GitHub Configuration ===\n",
    "       \n",
    "        # === Config ===\n",
    "        GITHUB_USERNAME = \"Langara-DataHub\"\n",
    "        GITHUB_TOKEN = \"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\"  # Use a fine-grained token with repo write access\n",
    "        REPO = \"Van-City-Project\"\n",
    "        BRANCH = \"develop\"\n",
    "        today_str = datetime.now().strftime(\"%Y%m%d\")\n",
    "        file_path = f\"/lakehouse/default/Files/forecast/{today_str}/{safe_name}\"\n",
    "        # === Read and encode image\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            content = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "        # === Create GitHub API URL\n",
    "        upload_path = f\"forecast_plots/{today_str}/{safe_name}\"\n",
    "        api_url = f\"https://api.github.com/repos/{GITHUB_USERNAME}/{REPO}/contents/{upload_path}\"\n",
    "\n",
    "        # === Commit message and headers\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {GITHUB_TOKEN}\",\n",
    "            \"Accept\": \"application/vnd.github+json\"\n",
    "        }\n",
    "\n",
    "        # === Get SHA if file exists\n",
    "        sha = None\n",
    "        check = requests.get(f\"{api_url}?ref={BRANCH}\", headers=headers)\n",
    "        if check.status_code == 200:\n",
    "            sha = check.json().get(\"sha\")\n",
    "\n",
    "        data = {\n",
    "            \"message\": f\"Add forecast plot for {park_name} - {datetime.now().strftime('%Y-%m-%d')}\",\n",
    "            \"content\": content,\n",
    "            \"branch\": BRANCH\n",
    "        }\n",
    "\n",
    "        if sha:\n",
    "            data[\"sha\"] = sha  # Required for overwrite\n",
    "\n",
    "        # === Push to GitHub\n",
    "        res = requests.put(api_url, headers=headers, json=data)\n",
    "\n",
    "        if res.status_code in [200, 201]:\n",
    "            print(f\"✅ Successfully pushed or updated: {upload_path}\")\n",
    "        else:\n",
    "            print(f\"❌ Failed to push: {res.status_code}, {res.json()}\")\n",
    "        \n",
    "        # Close to free memory\n",
    "        plt.close(fig)\n",
    "    \n",
    "    # AUTO-SAVE COMPLETION MESSAGE\n",
    "    if save_to_lakehouse:\n",
    "        print(\"Save Completed\")\n",
    "    \n",
    "    # STEP 3: Simplified Summary with Overall Confidence\n",
    "    print(\"\\n OPERATIONAL HOURS FORECAST SUMMARY\")\n",
    "    print(\"─\" * 60)\n",
    "    \n",
    "    for park_name, result in results.items():\n",
    "        df = result['data']\n",
    "        future_mask = result['future_mask']\n",
    "        \n",
    "        hist_data = df[~future_mask]['PopularTimesLivePercent']\n",
    "        pred_data = df[future_mask]['PopularTimesLivePercent']\n",
    "        \n",
    "        # Use smoothed historical data for trend comparison\n",
    "        if len(hist_data) > 0:\n",
    "            hist_smoothed = hist_data.rolling(window=3, center=True, min_periods=1).mean()\n",
    "            hist_avg_for_comparison = hist_smoothed.mean()\n",
    "        else:\n",
    "            hist_avg_for_comparison = 0\n",
    "        \n",
    "        if len(pred_data) > 0:\n",
    "            pred_hourly = df[future_mask].groupby(df[future_mask].index.hour)['PopularTimesLivePercent'].mean()\n",
    "            peak_hour = pred_hourly.idxmax()\n",
    "            peak_value = pred_hourly.max()\n",
    "            low_hour = pred_hourly.idxmin()\n",
    "            low_value = pred_hourly.min()\n",
    "            \n",
    "            # Get simplified confidence metrics\n",
    "            conf_metrics = result.get('confidence_metrics', {})\n",
    "            avg_conf = conf_metrics.get('avg_confidence', 0)\n",
    "            \n",
    "            print(f\"\\n {park_name} (6AM-10PM Operations)\")\n",
    "            print(f\"   • Average occupancy: {pred_data.mean():.0f}%\")\n",
    "            print(f\"   • Peak hour: {peak_hour}:00 ({peak_value:.0f}%)\")\n",
    "            print(f\"   • Quiet hour: {low_hour}:00 ({low_value:.0f}%)\")\n",
    "            print(f\"   • Total forecast hours: {len(pred_data)}\")\n",
    "            print(f\"   • Overall confidence: {avg_conf:.0f}%\")\n",
    "            \n",
    "            # Simplified confidence interpretation\n",
    "            if avg_conf >= 75:\n",
    "                conf_level = \" High - Reliable for planning\"\n",
    "            elif avg_conf >= 60:\n",
    "                conf_level = \" Medium - Generally reliable\"\n",
    "            elif avg_conf >= 45:\n",
    "                conf_level = \" Moderate - Use with caution\"\n",
    "            else:\n",
    "                conf_level = \" Low - High uncertainty\"\n",
    "            \n",
    "            print(f\"   • Confidence level: {conf_level}\")\n",
    "    \n",
    "    print(\"\\n Confidence is based on prediction consistency, historical similarity, and trend smoothness.\")\n",
    "    \n",
    "    return results\n",
    "    #print(summary_df.head(3))\n",
    "    #print(f\"Total rows in summary: {len(summary_df)}\")\n",
    "\n",
    "def forecast_summary(forecast_results, park_attributes_df):\n",
    "    \"\"\"\n",
    "    Use GitHub PlotURL instead of OneLake\n",
    "    \"\"\"\n",
    "    today_str = datetime.now().strftime(\"%Y%m%d\")\n",
    "    processed_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    rows = []\n",
    "    for name, res in forecast_results.items():\n",
    "        try:\n",
    "            df = res['data']\n",
    "            future = res['future_mask']\n",
    "            hist = df[~future]['PopularTimesLivePercent']\n",
    "            pred = df[future]['PopularTimesLivePercent']\n",
    "            if pred.empty: \n",
    "                continue\n",
    "\n",
    "            hist_avg = hist.rolling(3, center=True, min_periods=1).mean().mean() if not hist.empty else 0\n",
    "            pred_hourly = df[future].groupby(df[future].index.hour)['PopularTimesLivePercent'].mean()\n",
    "            conf = res.get('confidence_metrics', {}).get('avg_confidence', 0)\n",
    "            conf_cat = \"High\" if conf >= 75 else \"Medium\" if conf >= 60 else \"Moderate\" if conf >= 45 else \"Low\"\n",
    "\n",
    "            safe_filename = name.replace(\"/\", \"-\").replace(\" \", \"_\").replace(\",\", \"\") + \".png\"\n",
    "\n",
    "            row = {\n",
    "                'ParkName': name,\n",
    "                'ForecastDate': datetime.now().strftime('%Y-%m-%d'),\n",
    "                'ForecastStartDate': pred.index.min().strftime('%Y-%m-%d'),\n",
    "                'ForecastEndDate': pred.index.max().strftime('%Y-%m-%d'),\n",
    "                'AvgOccupancy': round(pred.mean(), 1),\n",
    "                'PeakHour': int(pred_hourly.idxmax()),\n",
    "                'PeakOccupancy': round(pred_hourly.max(), 1),\n",
    "                'LowHour': int(pred_hourly.idxmin()),\n",
    "                'LowOccupancy': round(pred_hourly.min(), 1),\n",
    "                'TotalForecastHours': len(pred),\n",
    "                'HistoricalAvg': round(hist_avg, 1),\n",
    "                'ConfidenceScore': round(conf, 1),\n",
    "                'ConfidenceCategory': conf_cat,\n",
    "                'PlotFilename': safe_filename,\n",
    "                'ProcessedDateTime': processed_time,\n",
    "                'PlotURL': f\"https://raw.githubusercontent.com/Langara-DataHub/Van-City-Project/develop/forecast_plots/{today_str}/{safe_filename}\"\n",
    "            }\n",
    "        \n",
    "            rows.append(row)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing forecast summary for {name}: {e}\")\n",
    "        \n",
    "    summary_df = pd.DataFrame(rows)\n",
    "    final_df = summary_df.merge(park_attributes_df[['ParkName', 'ParkKey']], on='ParkName', how='left')\n",
    "    return final_df\n",
    "\n",
    "# Usage\n",
    "forecast_results = create_forecast(merged_forecast_dict, park_last_known_dict, save_to_lakehouse=True)\n",
    "dim_park_attributes = spark.read.table(\"dim_park_attributes\").toPandas()\n",
    "\n",
    "summary_df = forecast_summary(forecast_results, dim_park_attributes)\n",
    "try:\n",
    "    print(summary_df.dtypes)\n",
    "    print(summary_df.head(1))\n",
    "    spark.createDataFrame(summary_df).write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"live_times_forecast_summary\")\n",
    "    print(\"Forecast summary table saved.\")\n",
    "except Exception as e:\n",
    "    print(\"Error during saveAsTable:\")\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac1b9cc-eb2f-4f4b-bc61-101ee0a0100e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-07T00:00:05.985504Z",
       "execution_start_time": null,
       "livy_statement_state": null,
       "normalized_state": "cancelled",
       "parent_msg_id": "1f680d8b-5253-4340-8e87-6a0e4326f8d4",
       "queued_time": "2025-08-06T23:55:52.3318459Z",
       "session_id": "da73cb9d-a63b-4402-8289-ae75c02491ac",
       "session_start_time": null,
       "spark_pool": null,
       "state": "cancelled",
       "statement_id": -1,
       "statement_ids": null
      },
      "text/plain": [
       "StatementMeta(, da73cb9d-a63b-4402-8289-ae75c02491ac, -1, Cancelled, , Cancelled)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_static_predictions(forecast_results):\n",
    "    \"\"\"\n",
    "    Extract all prediction results with features in a simple table format\n",
    "    \"\"\"\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for park_name, result in forecast_results.items():\n",
    "        df = result['data'].copy()\n",
    "        future_mask = result['future_mask']\n",
    "        \n",
    "        # Get prediction data only\n",
    "        predictions = df[future_mask].copy()\n",
    "        if predictions.empty:\n",
    "            continue\n",
    "            \n",
    "        # Add park name and reset index\n",
    "        predictions = predictions.reset_index()\n",
    "        predictions['ParkName'] = park_name\n",
    "        \n",
    "        # Get the actual datetime column name (first column after reset_index)\n",
    "        datetime_col_name = predictions.columns[0]\n",
    "        \n",
    "        # Add confidence if available\n",
    "        confidence = result.get('confidence_metrics', {}).get('avg_confidence', 0)\n",
    "        predictions['Confidence'] = confidence\n",
    "        \n",
    "        all_data.append(predictions)\n",
    "    \n",
    "    # Combine all parks\n",
    "    if not all_data:\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    static_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Get the actual datetime column name (should be consistent across all parks)\n",
    "    datetime_col = static_df.columns[0]  # First column is the datetime\n",
    "    \n",
    "    # Reorder columns: Park, Time, Target, Features\n",
    "    cols = ['ParkName', datetime_col, 'PopularTimesLivePercent', 'Confidence']\n",
    "    feature_cols = ['Temp', 'IsRaining', 'IsSnowing', 'IsHoliday', 'IsWeekend', 'HasEvent',\n",
    "                   'HourOp', 'DayOfWeek']\n",
    "    \n",
    "    # Add existing feature columns\n",
    "    for col in feature_cols:\n",
    "        if col in static_df.columns:\n",
    "            cols.append(col)\n",
    "    \n",
    "    static_df = static_df[cols]\n",
    "    static_df = static_df.sort_values(['ParkName', datetime_col]).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Extracted {len(static_df):,} predictions for {static_df['ParkName'].nunique()} parks\")\n",
    "    return static_df\n",
    "\n",
    "# USAGE\n",
    "\n",
    "# Extract static predictions\n",
    "static_predictions = get_static_predictions(forecast_results)\n",
    "\n",
    "# Preview\n",
    "print(f\"\\nShape: {static_predictions.shape}\")\n",
    "print(f\"Columns: {list(static_predictions.columns)}\")\n",
    "datetime_col = static_predictions.columns[1]  # Second column after ParkName\n",
    "print(f\"Date range: {static_predictions[datetime_col].min()} to {static_predictions[datetime_col].max()}\")\n",
    "print(\"\\nSample data:\")\n",
    "print(static_predictions.head(10))\n",
    "\n",
    "# Save to Delta table\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "table_name = f\"live_times_forecast_details_{timestamp.replace('_', '')}\"\n",
    "spark.createDataFrame(static_predictions).write.mode(\"overwrite\").format(\"delta\").saveAsTable(table_name)\n",
    "print(f\"\\n Saved to Delta table: {table_name}\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "e5e123bf-911a-451b-a899-c804c4a6fcc1",
    "default_lakehouse_name": "ParkBoard_Lakehouse",
    "default_lakehouse_workspace_id": "e622d16f-5a82-4c98-8a5b-b1447db03372",
    "known_lakehouses": [
     {
      "id": "e5e123bf-911a-451b-a899-c804c4a6fcc1"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
